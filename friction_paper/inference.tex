\subsection{Bayesian Inference}
 \label{sec:inference}
 
Bayesian inference is a statistical approach to inverse problems
that has recently gained great interest in different applications, including
ocean~\citep{Alexagnderian2011a,Zedler2012,sraj:2013a}
climate~\citep{OlsonEtAl2012} and geophysical~\citep{Malinverno2002} modeling.
The key idea in this approach is to express all forms of uncertainty
in terms of probability. The first step in Bayesian inference 
is to formulate the forward problem (response surface/model) using 
a suitable likelihood function and a product of conditional probability densities. 
The prior distributions over the unknown parameters of the model
are then formulated using the best available knowledge of the parameters.  Given some observation data, Bayes rule 
is then used to computer the posterior distribution for these unknown parameters
\citep{sivia}. We briefly review this approach below.
\subsubsection{Formalism}

Let $\vec{d}=(d_1,...,d_m)^T$ be a vector of observation data and $\vec{\theta}=(\theta_1,...,\theta_n)^T$ be a vector of model parameters or inputs. We consider a forward model $\vec G$ that predicts the data function of 
the parameters such that:

\begin{equation}
\vec d \approx \vec{G}( \vec \theta).
\end{equation}
Applying the Bayes' rule yields:

\begin{equation}
 \Pi(\vec{\theta}| \vec d) \propto 
 L(\vec d | \vec{\theta}) \ q(\vec{\theta}), 
\label{eq:bayes}
\end{equation}
where $q(\vec{\theta})$ is the prior of $\vec{\theta}$, representing the \emph{a priori} knowledge
about the parameters; 
$L(\vec d| \vec{\theta})$ is the likelihood function representing
the probability of obtaining the data given the set of parameters $\vec{\theta}$;
and finally $\Pi(\vec{\theta}| \vec d)$ is the posterior,
representing the probability of occurrence of $\vec{\theta}$ given the data $\vec d $.

To formulate the likelihood function, we let $\vec \epsilon = \vec d - \vec{G}$
represent the discrepancy between the model and observations.
Here, the components of $\vec \epsilon $ are assumed to be i.i.d. random variables with density $p_{\epsilon}$.
The likelihood function can thus be written as:

\begin{equation} 
L(\vec d |  \vec{\theta}) 
= 
\prod_{i=1}^m  
p_\epsilon (d_i - G_i(\vec \theta)).  	
\label{eq:likelihood}
\end{equation}

In our application, we assume that the errors $\epsilon_i$ are independent
and normally distributed with mean
zero and variance $\sigma^2$, i.e. $\epsilon_i \sim N(0,\sigma^2)$. 
While in general $\sigma^2$ (the variance of the noise in the measured data) depends on the observations, in our case we assume that the error
amplitude is generally small and does not change throughout space and time.
Consequently, one may assume a single $\sigma^2$ value in Equation~\eqref{eq:likelihood},
resulting in:

\begin{equation} 
L(\vec d |  \vec{\theta}) 
= 
\frac{1}{\sqrt{2 \pi \sigma^2}}\prod_{i=1}^m   
\exp \left\lbrace \frac{-(d_i - G_i(\vec \theta))^2}{2 \sigma^2} \right\rbrace. 	
\label{eq:likelihood2}
\end{equation}
The joint posterior from Bayes' rule can be given as:

\begin{equation} 
\Pi(\vec{\theta}| \vec d)
\propto
\frac{1}{\sqrt{2 \pi \sigma^2}}   \prod_{i=1}^m  
\exp \left\lbrace \frac{-(d_i - G_i(\vec \theta))^2}{2 \sigma^2} \right\rbrace  
\prod_{i=1}^n q(\theta_i).
\end{equation}
Since $\sigma^2$ is unknown \emph{a priori} we treat it as a hyper-parameter.
In other words, $\sigma^2$ becomes an additional parameter for Bayesian inference and  
therefore endowed with a prior which is updated based on available observations. In this 
case the joint posterior is finally expressed as:

\begin{equation} 
\Pi(\vec{\theta},\sigma^2 | \vec d)
\propto
\frac{1}{\sqrt{2 \pi \sigma^2}}   \prod_{i=1}^m  
\exp \left\lbrace \frac{-(d_i - G_i(\vec \theta))^2}{2 \sigma^2} \right\rbrace
\ q(\sigma^2) \prod_{i=1}^n q(\theta_i).
\label{eq:post}
\end{equation}

