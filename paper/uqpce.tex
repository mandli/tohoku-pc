\subsection{Accelerating Bayesian Inference}
\label{sec:uqpce}


MCMC exploration of the 4D posterior (Equation~\ref{eq:post_coef}) requires repeated simulations (tens of thousands) of the forward GeoClaw model, once for each proposed move of the Markov chain. While a single \emph{Geoclaw} simulation
takes $\sim 15~$mins, depending on the details of the MCMC sampler, it is desirable to avoid repeated forward solutions altogether. An alternative is to construct a surrogate model that requires a much
smaller ensemble of GeoClaw runs, and that can be used instead
at a significantly reduced computational cost.  Here, we rely on
Polynomial Chaos expansions for accelerating Bayesian inference in this context 
by building a surrogate, which, in addition
also provide statistical properties,  such as the mean, variance and sensitivities,
efficiently. 

\subsubsection{Polynomial Chaos}

Polynomial Chaos (PC) is a probabilistic methodology that expresses the 
dependencies of model outputs on the uncertain model inputs
as a polynomial truncated expansion. This method has been developed in 
the engineering community to represent uncertainties in the output of 
numerical simulations~\citep{Villegas2012,Lin2009,Xiu2004}
due to the uncertainties in a model's input. We briefly describe the PC
method below; for more details on this method, and other UQ methods
refer to \citep{LeMaitreKnio2010}.

Let $U=U(\bm{x},t,\xxi)$ denote a quantity of 
interest (QoI) that is the output of certain model.
$U$ is normally function of space $\bm{x}$ and time $t$, and 
also on the vector of normalized uncertain model inputs $\xxi=(\xi_1,...,\xi_n)$ (called the germ). PC expresses $U$ in the form:
\begin{equation}
  U(\xbold,t,\xxi) \doteq \sum_{k = 0}^P U_k(\xbold,t) \Psi_k(\xxi)
\label{eq:stochseries}
\end{equation} 
where $U_k(\xbold,t)$ are the polynomial coefficients, and
$\Psi_k(\xxi)$ are functions that form an orthogonal basis of an underlying probability
space. The total number of terms in the PC expansion is
$P+1 = \frac{(d+p)! }{n!\ p!}$ where $n$ is the number of stochastic dimension and $p$ is the highest order
of polynomials. 
This series representation can be viewed as a spectral expansion
of $U$ along the stochastic dimensions.  The existence and convergence of this series
is asserted by the Cameron-Martin theorem
\citep{Cameron:1947}  whenever $U$ has finite variance. The series rate of convergence, and
hence the number of terms to retain, depends on the smoothness of
$U$ with respect to $\xxi$. The series converges spectrally fast with $P$
when $U$ is infinitely smooth; the convergence rate becomes algebraic
when $U$ has finite smoothness \citep{Canuto:2006}. 
%In practice the series convergence is monitored 
%via various error metrics; Section \ref{sec:results}\ref{sec:analysis} presents one possible error
%analysis for the present problem.

The choice of basis is dictated by the probability density
function of the stochastic variable $\xxi$, and which appears as a weight
function in the stochastic space's inner product:
\begin{equation}
 \left<\Psi_i,\Psi_j\right> = \int \Psi_i(\xxi) \;\Psi_j(\xxi) \; \rho(\xxi) \; \mbox{d}\xxi=\delta_{ij}\ave{\Psi_i^2}
\label{eq:inner}
\end{equation}
where $\delta_{ij}$ is the Kronecker delta.
For uniform
distributions the basis functions are hence scaled Legendre polynomials.
For multi-dimensional problems the basis functions are
tensor products of 1D basis functions. The identification of the inner product weight function
with the probability distribution of $\xxi$ simplifies the calculations of $U$'s statistical moments.
Noting that since $\Psi_0(\xxi)$ is a constant that can be normalized to satisfy 
$\left<\Psi_0,\Psi_0\right>=1$, the expectation and variance of $U$ can be computed as:
\begin{eqnarray}
 E[U]&=&\int U \, \rho(\xxi) \, \mbox{d}\xxi=\left< U,\Psi_0\right> = U_0  
 \label{eq:mean} \\
 E[(U-E[U])^2]&=&\int (U-E[U])^2 \, \rho(\xxi) \, \mbox{d}\xxi=\sum_{k=1}^P U_k^2
 \label{eq:sigma}
\left<\Psi_k,\Psi_k\right>
\end{eqnarray}

PC representations of random variables also enable efficient
approximation of the corresponding variance-based, global
sensitivity indices. This can be done by computing the so-called {\it total} 
sensitivity indices of random variables from the PC representations or Sobol decomposition~\citep{Sobol:1993,Homma:1996,Sobol:2001}. To get the total sensitivity corresponding to the uncertain
input $\xi_i$ we compute the total index~\citep{LeMaitreKnio2010,Crestaux,Sudret}:
\begin{equation} \label{equ:T-hard}
   T_i = \displaystyle \sum_{u \ni i} S_u =
         \frac{\displaystyle
              \sum_{u \ni i} \sum_{k \in K_u} U_k^2 \ave{\Psi_k^2}}
              {\displaystyle\sum_{k = 1}^P U_k^2 \ave{\Psi_k^2}} .
\end{equation}
where for each index set $u \subseteq \{1, \ldots, n\}$ we define:
\[
   K_u = \left\{ k \in \{1, \ldots, P\} :
          \begin{cases}
           \vec{\alpha}^k_i > 0, \quad \mbox{ if } i \in u,\\
           \vec{\alpha}^k_i = 0, \quad \mbox{ if } i \notin u
          \end{cases}\right\}.
        \]
        and $\vec{\alpha}^k$ is the
multi-index associated with $k^{th}$ term in the
PC expansion~\cite{LeMaitreKnio2010}.

Using Equation~\eqref{equ:T-hard}, the computation of $T_i$ is straightforward.
Note that the total sensitivity index $T_i$ measures the contribution of
the $i^{th}$ random input to total model variability by
computing the fraction of the total variance due to all the terms in the
PC expansion which involve $\xi_i$. It is also worth noting that for random variables expanded in a PC basis, all the index sets
above are dictated by the basis alone. Thus, we need to find the index sets $K_u$
for computation of $S_u$ and $\mathcal{I}_i$ for computation
of $T_i$ only once. Hence, the computation of $S_u$ or $T_i$ for a
random variable is immediate once its PC coefficients are determined.


%The series representation (\ref{eq:stochseries}) can thus be seen as
%combining approximation and probabilistic frameworks, a combination
%that has proven extremely useful in solving UQ
%problems.



\subsubsection{Non Intrusive Spectral Projection (NISP)}
The computation of the coefficients of the PC expansions $U_k$
can be done using a number of procedures. Here we adopt a non-intrusive
approach that allows the use of the forward model \emph{Geoclaw} as a black box
with no code modifications required. PC expansion coefficients are thus determined
based on a set of response \emph{Geoclaw} simulations at specified set of the uncertain parameters. 
In particular, we rely on the Non-Intrusive Spectral Projection (NISP) method that exploits the orthogonality of the basis and applies the Galerkin projection to find the PC expansion coefficients as follows:
\begin{equation}
 U_k(\bm{x},t) = \frac{\left< U, \Psi_k \right>}{\left< \Psi_k, \Psi_k \right>} = 
 \frac{1}{\left< \Psi_k, \Psi_k \right>} 
 \int U(\bm{x},t,\xxi) \Psi_k(\xxi) \rho(\xxi) \mbox{ d}\xxi
\end{equation}
This orthogonal projection minimizes the L2 error on the space spanned by the basis.
with the inner-product indicated in Equation~\ref{eq:inner}.
Using NISP the stochastic integrals are solved using a numerical quadrature to obtain:

\begin{equation}
  \left< U, \Psi_k \right> 
\approx \left< U, \Psi_k \right>_Q
= \sum_{q=1}^Q U(\xxi_q) \Psi_k(\xxi_q) \omega_q
\end{equation}
where the subscript $Q$ refers to approximating the inner product integral with
quadrature, and $\xxi_q$ and $\omega_q$ are multi-dimensional quadrature points and weights,
respectively.
The computation of the ${U}_k$ can thus be expressed as a matrix-vector product of the form:
\begin{equation} 
 U_k(\bm{x},t)=\sum_q \Pi_{kq} U(\bm{x},t,\xxi_q),\;\;\;
 \Pi_{kq}=\frac{\Psi_k(\xxi_q)\omega_q}{\left< \Psi_k, \Psi_k \right>}
\end{equation} 
where $\Pi_{kq}$ is the projection matrix and $U(\bm{x},t,\xxi_q)$ is obtained
from an ensemble of the deterministic model {\em realizations} with the uncertain parameters set at
the quadrature value $\xxi_q$. 

The only cost of the NISP is that of the ensemble calculation and its storage.
The calculation of the $U(\bm{x},t,\xxi_q)$ is the most expensive part of the
inference procedure; thus, reducing the number of sampling points
while maximizing their effectiveness is critical for the procedure's
efficiency.  The quadrature order should be commensurate with the
truncation order, and should be high enough to avoid aliasing artifacts.
The choice of quadrature rule is hence critical to the performance
of the PC (in its NISP version at least). In this work, we employ the 
tensorized Gaussian quadratures as their computational costs are affordable.

