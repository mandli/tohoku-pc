\subsection{Bayesian Inference}
 \label{sec:inference}
Bayesian inference is a statistical approach to inverse problems
that has recently gained much interest in different applications.
The key idea in this approach is to express all forms of uncertainty
in terms of probability. The first step in a Bayesian inference 
is to formulate the forward problem (response surface/model) using 
a suitable likelihood function and product of conditional probability densities. 
We then formulate the prior distribution over the unknown parameters of the model
using our best belief. Lastly, given some observation data, Bayes rule 
is used to obtain a posterior distribution for these unknown
\citep{sivia}. We review this approach briefly below.

Let the observation data (water surface elevation at the gauges) be denoted by \{$S_i = S_i(x_i,y_i,t_i)\}_{i=1}^m$;
their model predicted counterparts by \{$M_i= M_i(x_i,y_i,t_i)\}_{i=1}^m$
($m$ is the number of observations) and $\{N_i\}_{i=1}^3$ the set of 
three Manning Friction coefficients treated as the unknown parameters. 
Bayes' rule yields:
\begin{equation}
 p(\{N_i\}_{i=1}^3| \{S_i\}_{i=1}^m) \propto 
 p(\{S_i\}_{i=1}^m | \{N_i\}_{i=1}^3) \ p(\{N_i\}_{i=1}^3)  
\label{eq:bayes}
\end{equation}
where $p(\{N_i\}_{i=1}^3)$ is the prior of $\{N_i\}_{i=1}^3$, representing the \emph{a priori} knowledge
about the manning coefficient; 
$p(\{S_i\}_{i=1}^m | \{N_i\}_{i=1}^3)$ is the likelihood function representing
the probability of obtaining the data given the set of parameters $\{N_i\}_{i=1}^3$;
and finally $p(\{N_i\}_{i=1}^3| \{ S_i \}_{i=1}^m)$ is the posterior,
representing the probability that $\{N_i\}_{i=1}^3$ is true given the data $( \{ S_i \}_{i=1}^m )$.

\begin{equation}
p(\{N_i\}_{i=1}^3 | \{ S_i \}_{i=1}^m)
\propto 
p(\{S_i\}_{i=1}^m | \{N_i\}_{i=1}^3) \ p(N_1)p(N_2) p(N_3)  
\end{equation}

To formulate the likelihood function, we let 
	 $\epsilon_i = S_i - M_i $
represents the discrepancy between the model and observations 
and assume that the observations are independent
and normally distributed with mean
zero and variance $\sigma^2$, i.e. 
$\epsilon_i \sim N(0,\sigma^2)$. In this 
case the likelihood function can be written as:
\begin{equation} 
p(\{S_i\}_{i=1}^m |  \{N_i\}_{i=1}^3) 
= 
\prod_{i=1}^m  \frac{1}{\sqrt{2 \pi \sigma^2}} 
\exp \left( \frac{-(S_i - M_i)^2}{2 \sigma^2} \right)  	
\label{eq:likelihood}
\end{equation}

The variance $\sigma^2$ is unknown \emph{a priori}; thus we treat it as a hyper-parameter.
While in general $\sigma^2$ depends on the observations, in cases where the error
amplitude is generally small and does not change throughout space and time, one may use
a simplified model and assume single hyper-parameter $\sigma^2$.

The next step is choosing the prior that should be based 
on some \emph{a priori} knowledge about the parameters. In our case, a uniform
prior for the model parameters is assumed:
\begin{equation} 
p(N_i) = \begin{cases}
		\displaystyle \frac{1}{b_i-a_i} &\text{for~} a_i <  N_i \leq b_i ,  \\
		0 &\text{otherwise}  , 
\end{cases}
\end{equation}
where $ [a_i,b_i]$ denote the parameter ranges defined in 
Equations~(\ref{eq:prior1}--\ref{eq:prior3}).
Regarding the variance, the only information we know 
is that $\sigma^2$ is always positive.
We thus assume a Jeffreys prior \citep{sivia}, expressed as:
\begin{equation} 
p(\sigma^2) =  \begin{cases}
		\displaystyle \frac{1}{\sigma^2} &\text{for~} \sigma^2 > 0,  \\
		0 &\text{otherwise}. 
		\end{cases}
\label{eq:var_pr}
\end{equation}
Consequently, Bayes' theorem gives:
\begin{equation} 
p(\{N_i\}_{i=1}^3,\sigma^2 | \{S_i\}_{i=1}^m) 
\propto
\left[ \prod_{i=1}^m  \frac{1}{\sqrt{2 \pi \sigma^2}} 
\exp \left( \frac{-(S_i - M_i)^2}{2 \sigma^2} \right) \right] 
\ p(\sigma^2)p(N_1)p(N_2) p(N_3)
\end{equation}
where $p(\{N_i\}_{i=1}^3,\sigma^2 | \{S_i\}_{i=1}^m)$  is the joint posterior.

Inferring the drag coefficient parameters requires 
sampling the posterior. In general, when the space of the unknown 
parameters is multidimensional, a suitable computational strategy is 
the Markov Chain Monte  Carlo (MCMC) method. 
We rely on an adaptive Metropolis MCMC \citep{Gareth2009,Haario2001} to
sample the posterior distribution accurately and efficiently.
This MCMC phase requires repeated (tens of
thousands of) Geoclaw simulations initialized with different values of the
uncertain parameters; this step is prohibitively expensive. An
alternative is to construct a surrogate model that requires a much
smaller ensemble of GeoClaw runs, and that can be used instead
at a significantly reduced computational cost.  Here, we rely on
PCEs to build the surrogate, which, in addition
also provide statistical properties,  such as the mean, variance and sensitivities,
efficiently.