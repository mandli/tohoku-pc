\subsection{Bayesian Inference}
 \label{sec:inference}
 
Bayesian inference is a statistical approach to inverse problems
that has recently gained great interest in different applications, including
ocean~\citep{Alexanderian2011a,Zedler2012,sraj:2013a}
climate~\citep{OlsonEtAl2012} and geophysical~\citep{Malinverno2002} modeling.
The key idea in this approach is to express all forms of uncertainty
in terms of probability. The first step in Bayesian inference 
is to formulate the forward problem (response surface/model) using 
a suitable likelihood function and a product of conditional probability densities. 
The prior distributions over the unknown parameters of the model
are then formulated using our best belief. Lastly, given some observation data, Bayes rule 
is used to obtain a posterior distribution for these unknown parameters
\citep{sivia}. We review this approach briefly below.
\subsubsection{Formalism}

Let $\vec{d}=(d_1,...,d_m)^T$ be a vector of observation data and $\vec{\theta}=(\theta_1,...,\theta_n)^T$ be a vector of model parameters or inputs. We consider a forward model $\vec G$ that predicts the data function of 
the parameters such that:

\begin{equation}
\vec d \approx \vec{G}( \vec \theta).
\end{equation}
Applying the Bayes' rule yields:

\begin{equation}
 p(\vec{\theta}| \vec d) \propto 
 p(\vec d | \vec{\theta}) \ p(\vec{\theta}), 
\label{eq:bayes}
\end{equation}
where $p(\vec{\theta})$ is the prior of $\vec{\theta}$, representing the \emph{a priori} knowledge
about the parameters; 
$p(\vec d| \vec{\theta})$ is the likelihood function representing
the probability of obtaining the data given the set of parameters $\vec{\theta}$;
and finally $p(\vec{\theta}| \vec d)$ is the posterior,
representing the probability of occurrence of $\vec{\theta}$ given the data $\vec d $.

To formulate the likelihood function, we let $\vec \epsilon = \vec d - \vec{G}$
represents the discrepancy between the model and observations.
Here, the components of $\vec \epsilon $ are assumed to be i.i.d. random variables with density $p_{\epsilon}$.
In this case the likelihood function can be written as:

\begin{equation} 
p(\vec d |  \vec{\theta}) 
= 
\prod_{i=1}^m  
p_\epsilon (d_i - G_i(\vec \theta)).  	
\label{eq:likelihood}
\end{equation}

In our application, we assume that the errors $\epsilon_i$ are independent
and normally distributed with mean
zero and variance $\sigma^2$, i.e. 
$\epsilon_i \sim N(0,\sigma^2)$. 
While in general $\sigma^2$ depends on the observations, in cases where the error
amplitude is generally small and does not change throughout space and time, one may use
a simplified model and assume single $\sigma^2$ value.
Consequently, Equation~\eqref{eq:likelihood} becomes:

\begin{equation} 
p(\vec d |  \vec{\theta}) 
= 
\frac{1}{\sqrt{2 \pi \sigma^2}}\prod_{i=1}^m   
\exp \left\lbrace \frac{-(d_i - G_i(\vec \theta))^2}{2 \sigma^2} \right\rbrace. 	
\label{eq:likelihood2}
\end{equation}
The joint posterior from Bayes' rule can be given as:

\begin{equation} 
p(\vec{\theta}| \vec d)
\propto
\frac{1}{\sqrt{2 \pi \sigma^2}}   \prod_{i=1}^m  
\exp \left\lbrace \frac{-(d_i - G_i(\vec \theta))^2}{2 \sigma^2} \right\rbrace  
\prod_{i=1}^n p(\theta_i).
\end{equation}
The variance $\sigma^2$ is unknown \emph{a priori}; thus we treat it as a hyper-parameter.
In other words, $\sigma^2$ may become an additional parameter for Bayesian inference and  
therefore endowed with a prior and estimated from data. In this case the joint posterior is finally written as follows:

\begin{equation} 
p(\vec{\theta},\sigma^2 | \vec d)
\propto
\frac{1}{\sqrt{2 \pi \sigma^2}}   \prod_{i=1}^m  
\exp \left\lbrace \frac{-(d_i - G_i(\vec \theta))^2}{2 \sigma^2} \right\rbrace
\ p(\sigma^2) \prod_{i=1}^n p(\theta_i).
\label{eq:post}
\end{equation}

